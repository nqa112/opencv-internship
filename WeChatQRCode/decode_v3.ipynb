{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python==4.5.2.52 in /Users/Quoc_Anh/.local/lib/python3.8/site-packages (4.5.2.52)\n",
      "Requirement already satisfied: opencv-contrib-python==4.5.2.52 in /Users/Quoc_Anh/.local/lib/python3.8/site-packages (4.5.2.52)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/Quoc_Anh/.conda/envs/vision/lib/python3.8/site-packages (from opencv-contrib-python==4.5.2.52) (1.23.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pillow==9.1.0 in /Users/Quoc_Anh/.conda/envs/vision/lib/python3.8/site-packages (9.1.0)\n",
      "Requirement already satisfied: torch==1.10.2 in /Users/Quoc_Anh/.conda/envs/vision/lib/python3.8/site-packages (1.10.2)\n",
      "Requirement already satisfied: torchvision==0.11.3 in /Users/Quoc_Anh/.conda/envs/vision/lib/python3.8/site-packages (0.11.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/Quoc_Anh/.conda/envs/vision/lib/python3.8/site-packages (from torch==1.10.2) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in /Users/Quoc_Anh/.conda/envs/vision/lib/python3.8/site-packages (from torchvision==0.11.3) (1.23.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm==4.64.1 in /Users/Quoc_Anh/.conda/envs/vision/lib/python3.8/site-packages (4.64.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Environment using Python 3.8.11\n",
    "%pip install opencv-python==4.5.2.52 opencv-contrib-python==4.5.2.52 # Install compatible OpenCV version with WeChatQRCode\n",
    "%pip install pillow==9.1.0 torch==1.10.2 torchvision==0.11.3\n",
    "%pip install tqdm==4.64.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Choose device for model running, the reconstruction is performed on the CPU by default\n",
    "# This can be changed by simply replacing .cpu() with .to(device)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upscaling image using SRCNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolution(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    SRCNN Network Architecture as per specified in the paper. \n",
    "    The chosen configuration for successive filter sizes are 9-5-5\n",
    "    The chosed configuration for successive filter depth are 128-64(-3)\n",
    "    \"\"\"\n",
    "    def __init__(self, sub_image: int = 33, spatial: list = [9, 5, 5], filter: list = [128, 64], num_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Conv2d(num_channels, filter[0], spatial[0], padding = spatial[0] // 2)\n",
    "        self.layer_2 = nn.Conv2d(filter[0], filter[1], spatial[1], padding = spatial[1] // 2)\n",
    "        self.layer_3 = nn.Conv2d(filter[1], num_channels, spatial[2], padding = spatial[2] // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, image_batch):\n",
    "        x = self.layer_1(image_batch)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        y = self.relu(x)\n",
    "        x = self.layer_3(y)\n",
    "        return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(image_in, model, fs = 33, scale = None):\n",
    "  \n",
    "    \"\"\"\n",
    "    Executes the model trained on colab, on any image given (link or local), with an \n",
    "    upscaling factor as mentioned in the arguments. For best results, use a scale of\n",
    "    2 or lesser, since the model was trained on a scale of 2\n",
    "    Inputs : image_in               -> torch.tensor representing the image, can be easily obtained from \n",
    "                                       transform_image function in this script (torch.tensor)\n",
    "             model                  -> The trained model, trained using the same patch size \n",
    "                                       (object of the model class, inherited from nn.Module) \n",
    "             fs                     -> Patch size, on which the model is run (int)\n",
    "             scale                  -> Scale on which the image is upscaled (float) \n",
    "    Outputs: reconstructed_image    -> The higher definition image as output (torch.tensor)\n",
    "    \"\"\"\n",
    "    # Write the transforms and prepare the empty array for the image to be written\n",
    "    c, h, w = image_in.shape\n",
    "    scale_transform = transforms.Resize((int(h * scale), int(w * scale)), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    image = to_tensor(scale_transform(to_pil(image_in)))\n",
    "    n = 0\n",
    "    c, h, w = image.shape\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    reconstructed_image = torch.zeros_like(image).cpu()\n",
    "    reconstructed_image_weights = torch.zeros_like(image).cpu()\n",
    "\n",
    "    # Loop for non overlapping image reconstruction \n",
    "    # since overlapping reconstruction needs too much memory even for small images \n",
    "    for i in range(h // fs):\n",
    "      for j in range(w // fs):\n",
    "\n",
    "        # Clean up memory and track iterations\n",
    "        gc.collect()\n",
    "        n += 1\n",
    "\n",
    "        # Get the j'th (fs, fs) shaped patch of the (i * fs)'th row, \n",
    "        # Upscale this patch and write to the empty array at appropriate location  \n",
    "        patch = image[:, :, i * fs: i * fs + fs, j * fs: j * fs + fs]\n",
    "        reconstructed_image[:, :, i * fs: i * fs + fs, j * fs: j * fs + fs] = model(patch)[0].cpu().clamp(0, 1)\n",
    "        reconstructed_image_weights[:, :, i * fs: i * fs + fs, j * fs: j * fs + fs] += torch.ones(1, c, fs, fs)\n",
    "        \n",
    "        # This leaves the right and bottom edge black, if the width and height are not divisible by fs\n",
    "        # Those edge cases are dealt with here\n",
    "        if j == w // fs - 1:\n",
    "            patch = image[:, :, i * fs: i * fs + fs, w - fs: w]\n",
    "            reconstructed_image[:, :, i * fs: i * fs + fs, w - fs: w] = model(patch)[0].cpu().clamp(0, 1)\n",
    "        if i == h // fs - 1:\n",
    "            patch = image[:, :, h - fs: h, j * fs: j * fs + fs]\n",
    "            reconstructed_image[:, :, h - fs: h, j * fs: j * fs + fs] = model(patch)[0].cpu().clamp(0, 1)\n",
    "          \n",
    "      # Make the right bottom patch, since none of the edge cases have covered it\n",
    "      patch = image[:, :, h - fs: h, w - fs: w]\n",
    "      reconstructed_image[:, :, h - fs: h, w - fs: w] = model(patch)[0].cpu().clamp(0, 1)\n",
    "    \n",
    "    return reconstructed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upscaled output is obtained and converted to OpenCV Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale(path_to_image, path_to_model):\n",
    "\n",
    "    # Instantiate model and load state dict using .pth file \n",
    "    model = SuperResolution()\n",
    "    if torch.cuda.is_available():\n",
    "        model.load_state_dict(torch.load(path_to_model))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(path_to_model, map_location={'cuda:0': 'cpu'}))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Run the progressive scan to increase resolution of the image \n",
    "    image = Image.open(path_to_image)\n",
    "    trans = transforms.Compose([transforms.ToTensor()])\n",
    "    transformed = trans(image)\n",
    "    reconstructed = execute(transformed, model, scale = 2)  \n",
    "    to_pil = transforms.ToPILImage()\n",
    "\n",
    "    # Convert upscaled image to PIL format\n",
    "    pil_upscaled = to_pil(reconstructed.squeeze())\n",
    "\n",
    "    # Convert PIL to OpenCV Matrix\n",
    "    opencv_upscaled = np.array(pil_upscaled) \n",
    "    # Convert RGB to BGR \n",
    "    opencv_upscaled = opencv_upscaled[:, :, ::-1].copy()\n",
    "\n",
    "    return opencv_upscaled "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating brightness and contrast enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertScale(img, alpha, beta):\n",
    "    # Add bias and gain to an image with saturation arithmetics. Unlike\n",
    "    # cv2.convertScaleAbs, it does not take an absolute value, which would lead to\n",
    "    # nonsensical results (e.g., a pixel at 44 with alpha = 3 and beta = -210\n",
    "    # becomes 78 with OpenCV, when in fact it should become 0).\n",
    "\n",
    "    new_img = img * alpha + beta\n",
    "    new_img[new_img < 0] = 0\n",
    "    new_img[new_img > 255] = 255\n",
    "    return new_img.astype(np.uint8)\n",
    "\n",
    "# Automatic brightness and contrast optimization with optional histogram clipping\n",
    "def autoBrightnessAndContrast(image, clip_hist_percent):\n",
    "\n",
    "    # Calculate grayscale histogram\n",
    "    hist = cv2.calcHist([image],[0],None,[256],[0,256])\n",
    "    hist_size = len(hist)\n",
    "\n",
    "    # Calculate cumulative distribution from the histogram\n",
    "    accumulator = []\n",
    "    accumulator.append(float(hist[0]))\n",
    "    for index in range(1, hist_size):\n",
    "        accumulator.append(accumulator[index -1] + float(hist[index]))\n",
    "\n",
    "    # Locate points to clip\n",
    "    maximum = accumulator[-1]\n",
    "    clip_hist_percent *= (maximum/100.0)\n",
    "    clip_hist_percent /= 2.0\n",
    "\n",
    "    # Locate left cut\n",
    "    minimum_gray = 0\n",
    "    while accumulator[minimum_gray] < clip_hist_percent:\n",
    "        minimum_gray += 1\n",
    "\n",
    "    # Locate right cut\n",
    "    maximum_gray = hist_size -1\n",
    "    while accumulator[maximum_gray] >= (maximum - clip_hist_percent):\n",
    "        maximum_gray -= 1\n",
    "\n",
    "    # Calculate alpha and beta values\n",
    "    alpha = 255 / (maximum_gray - minimum_gray)\n",
    "    beta = -minimum_gray * alpha\n",
    "\n",
    "    auto_result = convertScale(image, alpha=alpha, beta=beta)\n",
    "    return auto_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw input image is ready to be enhanced and decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QRreader(path_to_image, path_to_model, imgsz):\n",
    "\n",
    "    # declare QR result variable\n",
    "    qrData = None\n",
    "    \n",
    "    # enhance image to super resolution\n",
    "    upscaled_image = upscale(path_to_image, path_to_model)\n",
    "\n",
    "    # gray scale for faster computation\n",
    "    gray = cv2.cvtColor(upscaled_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if gray is not None:\n",
    "\n",
    "        # stop program if no QR code detected\n",
    "        try:\n",
    "            h, w = gray.shape[:2]  \n",
    "            \n",
    "            # resize for faster decoding with optional value\n",
    "            ratio = imgsz / w\n",
    "            resize = cv2.resize(gray, (imgsz, round(h * ratio)), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "            # deblur\n",
    "            gaussian = cv2.GaussianBlur(resize, (15, 15), 10.0)\n",
    "            unsharp = cv2.addWeighted(resize, 8, gaussian, -7, 0)\n",
    "\n",
    "            # automatic brightness and contrast optimization with optional histogram clipping\n",
    "            brightness_contrast = autoBrightnessAndContrast(unsharp, 5)\n",
    "\n",
    "            # binarization\n",
    "            _, bin = cv2.threshold(brightness_contrast, 0, 255, cv2.THRESH_OTSU)\n",
    "            \n",
    "            # decode using OpenCV WechatQRCode\n",
    "            detector = cv2.wechat_qrcode_WeChatQRCode(\"./model/detect.prototxt\",\n",
    "                                                      \"./model/detect.caffemodel\",\n",
    "                                                      \"./model/sr.prototxt\",\n",
    "                                                      \"./model/sr.caffemodel\")\n",
    "\n",
    "            result = detector.detectAndDecode(bin)\n",
    "\n",
    "            if len(result[0]) != 0:\n",
    "                qrData = result[0][0]        \n",
    "        except:\n",
    "            qrData = None\n",
    "        \n",
    "    return qrData, bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding in progess :  16%|█▋        | 500/3039 [18:44<1:35:10,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECODE SUCCESSFULLY: 252/501\n",
      "RATE: 50.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Declare QR images and trained model directory\n",
    "sourcePath = \"qr_large\"\n",
    "superResModelPath = \"SRCNN/isr_best.pth\"\n",
    "# Resize image to 320 x 320\n",
    "# recommended value in range (250, 350)\n",
    "imgsz = 280\n",
    "\n",
    "# Count number of images scanned\n",
    "imgNum = 0 \n",
    "decodeSuccess = 0\n",
    "# Decode a series of QR code images in \"sourceFolder\"\n",
    "for f in tqdm(os.listdir(sourcePath), desc = \"Decoding in progess \"):\n",
    "    try:\n",
    "        imgPath = os.path.join(sourcePath, f)\n",
    "        result, img = QRreader(imgPath, superResModelPath, imgsz)\n",
    "        \n",
    "        if result is not None:\n",
    "            # print(f\"{result}\\n\")\n",
    "            decodeSuccess += 1\n",
    "        imgNum += 1\n",
    "    except:\n",
    "        raise AssertionError\n",
    "\n",
    "rate = decodeSuccess / imgNum * 100\n",
    "print(f\"DECODE SUCCESSFULLY: {decodeSuccess}/{imgNum}\\nRATE: {rate:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f141a2a29f904d4ff0b2467704ae9124c09f2a716e5018b070bc664232bdb6e0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
